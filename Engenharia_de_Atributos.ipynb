{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM82ZKG0+aKjRJd968YEP59",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irajamuller/data_science/blob/main/Engenharia_de_Atributos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Engenharia de Atributos**\n",
        "---"
      ],
      "metadata": {
        "id": "_464Inp80Tak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1-Codificação de Dados\n",
        "<p align=\"justify\">\n",
        "Este notebook irá explorar a utilização do Scikit-Learn para a codificação de variáveis categóricas em conjuntos de dados. A maioria dos algoritmos de aprendizado de máquina é compatível apenas com atributos numéricos, não sendo possível a utilização de variáveis categóricas sem algum tipo de preparação. Por isso, é comum o uso de métodos de codificação para tornar variáveis categóricas em algum tipo de variável numérica.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Iremos explorar dois tipos de codificação neste notebook: one-hot e por etiquetas. Ambos tipos de codificação são implementados como técnicas de pré-processamento disponíveis na biblioteca Scikit-Learn. Iremos utilizar ambas as técnicas para realizar a codificação de atributos categóricos do conjunto de dados `tips` disponível na biblioteca Seaborn. Primeiramente iremos importar as bibliotecas Pandas e Seaborn para manipular o conjunto de dados de exemplo.\n",
        "</p>"
      ],
      "metadata": {
        "id": "hgXZgkh30e-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "m1IJZiuM00FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O método one-hot será o primeiro que iremos trabalhar. Este método de codificação converte dados categóricos em uma forma numérica que possa ser usada por algoritmos de aprendizado de máquina. Cada categoria única de uma variável se torna um atributo binário onde 0 indica a ausência da categoria e 1 indica a presença.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Por exemplo, suponha que o conjunto de dados possua uma categoria \"gênero\" que classifica indivíduos como masculino ou feminino. Antes do processo de codificação, um exemplo de conjunto de dados seria:\n",
        "</p>\n",
        "\n",
        "<center>\n",
        "\n",
        "| Nome  | Gênero |\n",
        "|-------|--------|\n",
        "| João  | M      |\n",
        "| Maria | F      |\n",
        "\n",
        "</center>\n",
        "\n",
        "Após o processo de codificação, teríamos o seguinte conjunto de dados:\n",
        "\n",
        "<center>\n",
        "\n",
        "| Nome  | Gênero_M | Gênero_F |\n",
        "|-------|----------|----------|\n",
        "| João  | 1        | 0        |\n",
        "| Maria | 0        | 1        |\n",
        "\n",
        "</center>\n",
        "\n",
        "A célula abaixo apresenta um exemplo do uso de codificação one-hot sobre o conjunto de dados <strong>tips</strong>."
      ],
      "metadata": {
        "id": "UYRbczFO08Pb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpcAYDqMvcGU"
      },
      "outputs": [],
      "source": [
        "# Importar a classe OneHotEncoder do Scikit-Learn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Carregar o conjunto de dados \"tips\" através do Seaborn\n",
        "dataset = sns.load_dataset('tips')\n",
        "\n",
        "# Iremos criar um subconjunto de dados contendo apenas as colunas que irão passar pelo processo de codificação one-hot\n",
        "categorias = dataset[['sex', 'smoker', 'day', 'time']]\n",
        "\n",
        "#  Agora criamos a instância da classe OneHotEncoder e a utilizamos para criar a codificação dos atributos\n",
        "encoder = OneHotEncoder(sparse_output=False) # sparse_output (denso/esparso)\n",
        "categorias_encoded = encoder.fit_transform(categorias)\n",
        "\n",
        "# Após o processo de codificação, precisamos criar de forma explícita um dataframe contendo o resultado do processo\n",
        "feature_names = encoder.get_feature_names_out(categorias.columns)\n",
        "categorias_encoded_df = pd.DataFrame(categorias_encoded, columns=feature_names)\n",
        "\n",
        "# Por fim, podemos substituir os dados originais pelos novos atributos criados através da codificação.\n",
        "# Precisamos primeiramente \"alinhar\" os índices do conjunto de dados com a codificação one-hot com\n",
        "# aqueles do conjunto original. Em seguida, podemos remover as colunas com os atributos originais\n",
        "# pelos atributos criados pela codificação.\n",
        "categorias_encoded_df.index = dataset.index\n",
        "dataset = dataset.drop(columns=categorias.columns)\n",
        "dataset = pd.concat([dataset, categorias_encoded_df], axis=1)\n",
        "\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O próximo método com o qual iremos trabalhar é a codificação por etiquetas ou <strong>label encoding</strong>. Este método transforma dados categóricos em valores numéricos inteiros. Por exemplo, suponha que o conjunto de dados possua uma categoria \"gênero\" que classifica indivíduos como masculino ou feminino. Antes do processo de codificação, um exemplo de conjunto de dados seria:\n",
        "</p>\n",
        "\n",
        "<center>\n",
        "\n",
        "| Nome  | Gênero |\n",
        "|-------|--------|\n",
        "| João  | M      |\n",
        "| Maria | F      |\n",
        "\n",
        "</center>\n",
        "\n",
        "Após o processo de codificação, teríamos o seguinte conjunto de dados:\n",
        "\n",
        "<center>\n",
        "\n",
        "| Nome  | Gênero |\n",
        "|-------|--------|\n",
        "| João  | 1      |\n",
        "| Maria | 2      |\n",
        "\n",
        "</center>\n",
        "\n",
        "<p align=\"justify\">\n",
        "A codificação por etiquetas está disponível na classe <strong>LabelEncoder</strong> do Scikit-Learn. A célula abaixo apresenta um exemplo do uso deste tipo de codificação nos atributos do conjunto de dados <strong>tips</strong>.\n",
        "</p>"
      ],
      "metadata": {
        "id": "sCtBdoxe1uWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar a classe LabelEncoder do Scikit-Learn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Carregar o conjunto de dados \"tips\" através do Seaborn\n",
        "dataset = sns.load_dataset('tips')\n",
        "\n",
        "# Neste exemplo precisamos apenas criar uma lista com os atributos que passarão pelo processo de codificação\n",
        "categorias = ['sex', 'day']\n",
        "\n",
        "# Vamos criar uma instância do LabelEncoder para realizar o processo de codificação\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Iteramos pela lista de atributos que passarão pela codificação. Para cada atributo, criamos um novo\n",
        "# atributo no conjunto de dados, contendo a respectiva versão codificada.\n",
        "for col in categorias:\n",
        "    # Codificar os valores da coluna\n",
        "    dataset[f\"{col}_encoded\"] = label_encoder.fit_transform(dataset[col])\n",
        "\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "-f2N0Xx52Oin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2-Seleção de Atributos Categóricos - SelectKBest\n",
        "<p align=\"justify\">\n",
        "A seleção de atributos se trata de um conjunto de técnicas que permite identificar quais os atributos apresentam maior relevância para a predição de uma determinada variável alvo. Estes métodos envolvem técnicas estatísticas que analisam diferentes características do conjunto de dados e buscam estabelecer correlações entre os valores de diferentes atributos.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "A classe <strong>SelectKBest</strong> do Scikit-Learn implementa um processo de análise e seleção dos atributos de maior relevância em um conjunto de dados. Ela permite o uso de diferentes métodos estatísticos para análises de correlação, tais como o qui-quadrado ou o ANOVA. Cada um destes testes pode ser aplicado em diferentes contextos. Por exemplo, o teste do qui-quadrado pode ser aplicado para identificar a relevância de variáveis categóricas, mas não terá resultados aceitáveis no caso de variáveis contínuas.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Este notebook apresenta um exemplo da aplicação do teste de qui-quadrado através da classe <strong>SelectKBest</strong> em um subconjunto do dataset <strong>titanic</strong>, disponível através do Seaborn. Nosso objetivo é identificar qual das variáveis categóricas presentes nesse conjunto de dados apresenta maior relevância para a classificação da variável alvo <strong>survivor</strong>.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Primeiramente iremos carregar as bibliotecas que utilizaremos ao longo do nosso experimento. Além do Pandas e do Seaborn, que serão utilizados para carregar e manipular o conjunto de dados, também carregaremos módulos específicos da Scikit-Learn para uso da seleção de atributos.\n",
        "</p>"
      ],
      "metadata": {
        "id": "M6fItAE33l8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "Ont-LBCJ4TS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O próximo passo será carregar o conjunto de dados e realizar alguns tratamentos iniciais para permitir que a seleção de atributos seja executada. Em suma:\n",
        "</p>\n",
        "\n",
        "- Remoção das variáveis contínuas como <strong>age</strong> e <strong>fare</strong>;\n",
        "- Remoção das entradas com valores nulos;\n",
        "- Codificação das variáveis categóricas através do <strong>LabelEncoder</strong>."
      ],
      "metadata": {
        "id": "IIqRnmu24Vlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar conjunto de dados Titanic\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Pré-processamento:\n",
        "# - Selecionar colunas relevantes\n",
        "# - Remover valores ausentes\n",
        "titanic = titanic[['survived', 'pclass', 'sex', 'embarked']]\n",
        "titanic = titanic.dropna()\n",
        "\n",
        "# Codificar variáveis categóricas para numéricas\n",
        "encoder = LabelEncoder()\n",
        "titanic['sex'] = encoder.fit_transform(titanic['sex'])\n",
        "titanic['embarked'] = encoder.fit_transform(titanic['embarked'])"
      ],
      "metadata": {
        "id": "mMnmGDvL4k9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O próximo passo será dividir o conjunto de dados entre as variáveis preditoras (usadas para descoberta da classe) e a variável alvo (aquela que se deseja descobrir a classe).\n",
        "</p>"
      ],
      "metadata": {
        "id": "MrNzcDPb4nq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar variáveis preditoras (X) e alvo (y)\n",
        "X = titanic[['pclass', 'sex', 'embarked']]\n",
        "y = titanic['survived']"
      ],
      "metadata": {
        "id": "ztbz0tu54uWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A próxima célula instancia a classe `SelectKBest`, definindo o qui-quadrado (`chi2`) como teste estatístico a ser utilizado. Após a aplicação do teste, são extraídos os resultados do teste, os quais são inseridos em um Dataframe para análise ao final da célula. Os resultados obtidos com o teste possuem, para cada atributo, os seguintes valores:\n",
        "</p>\n",
        "\n",
        "- <strong>chi2 score</strong>: indica o quanto os valores observados de uma variável diferem dos valores esperados sob a hipótese nula. Valores mais altos neste resultado sugerem uma associação mais forte entre o atributo e a variável alvo.\n",
        "- <strong>p-value</strong>: é a probabilidade de observar um Chi2-Score tão extremo quanto o calculado assumindo que a hipótese nula seja verdadeira. Um p-value menor que 0,05 sugere que há associação entre o atributo e a variável e, portanto, o atributo é relevante para a predição."
      ],
      "metadata": {
        "id": "mcPjUrFs433_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleção de atributos com o teste qui-quadrado\n",
        "selector = SelectKBest(score_func=chi2, k='all')\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Obter pontuações do qui-quadrado e p-valores\n",
        "chi2_scores = selector.scores_\n",
        "chi2_pvalues = selector.pvalues_\n",
        "\n",
        "# Exibir resultados em um DataFrame\n",
        "df_chi2 = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Chi2-Score': chi2_scores,\n",
        "    'p-Value': chi2_pvalues\n",
        "}).sort_values(by='Chi2-Score', ascending=False)\n",
        "\n",
        "print(\"Resultados do Teste Qui-Quadrado:\")\n",
        "df_chi2"
      ],
      "metadata": {
        "id": "2Ddj2aWu5RJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.3-Seleção de Atributos Categóricos - RFE\n",
        "<p align=\"justify\">\n",
        "A eliminação recursiva de atributos (RFE ou <strong>random feature elimination</strong>) é outro método que podemos utilizar para identificar os atributos de maior relevância em um conjunto de dados. Ele funciona de maneira iterativa, treinando um modelo base e avaliando a importância de cada atributo. Em cada etapa, o atributo menos relevante é removido, e o modelo é re-treinado com os atributos restantes. Esse processo continua até que o número desejado de atributos seja alcançado. O RFE é útil para reduzir a dimensionalidade do conjunto de dados, melhorar a eficiência do modelo e minimizar o risco de overfitting, especialmente em problemas com muitos atributos redundantes ou irrelevantes.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Conforme mencionado, a operação do RFE requer o uso de um algoritmo de aprendizado de máquina mais simples, tal como uma árvore de decisão ou uma random forest. Modelos mais complexos também podem ser utilizados, mas sua complexidade pode afetar o tempo necessário para a obtenção do resultado.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Neste notebook iremos utiliza o método de RFE disponibilizado pela biblioteca Scikit-Learn em conjunto com um algoritmo de árvore de decisão. O processo será aplicado sobre o conjunto de dados `titanic`. Primeiramente iremos importar as bibliotecas Pandas e Seaborn para que possamos carregar e manipular o conjunto de dados. Também iremos carregar os módulos necessários do Scikit-Learn para a utilização do método de RFE.\n",
        "</p>"
      ],
      "metadata": {
        "id": "STWISJCS6FHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "NCbwMSUw6Rqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A célula a seguir realiza o carregamento e preparação do conjunto de dados. Os seguintes passos serão executados:\n",
        "</p>\n",
        "\n",
        "- Carregamento do conjunto de dados;\n",
        "- Seleção dos atributos de interesse dentro do conjunto\n",
        "- Eliminação dos registros com valores faltantes;\n",
        "- Codificação em etiquetas dos atributos categóricos <strong>sex</strong> e <strong>embarked</strong>."
      ],
      "metadata": {
        "id": "7QCb9AM86Wo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o dataset Titanic\n",
        "titanic = sns.load_dataset(\"titanic\")\n",
        "\n",
        "# Selecionar colunas relevantes e tratar valores ausentes\n",
        "titanic = titanic[['survived', 'pclass', 'sex', 'age', 'fare', 'embarked']]\n",
        "titanic = titanic.dropna()\n",
        "\n",
        "# Codificar variáveis categóricas\n",
        "encoder = LabelEncoder()\n",
        "titanic['sex'] = encoder.fit_transform(titanic['sex'])\n",
        "titanic['embarked'] = encoder.fit_transform(titanic['embarked'])"
      ],
      "metadata": {
        "id": "9-3DcCjI6VYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Assim como no uso do SelectKBest, para utilizar o RFE precisamos dividir o conjunto de dados entre os atributos preditores e o atributo alvo. A célula a seguir realiza esta divisão.\n",
        "</p>"
      ],
      "metadata": {
        "id": "dSBpirsV6jtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separar variáveis preditoras (X) e alvo (y)\n",
        "X = titanic[['pclass', 'sex', 'age', 'fare', 'embarked']]\n",
        "y = titanic['survived']"
      ],
      "metadata": {
        "id": "IO7MuXmR6rC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Após a preparação do conjunto de dados, podemos aplicar o método de RFE utilizando a árvore de decisão. A célula abaixo instancia as classes necessárias para aplicar o método RFE e, em seguida, prepara um Dataframe para exibição dos resultados do processamento.\n",
        "</p>"
      ],
      "metadata": {
        "id": "NiMo0wkr6uBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Primeiramente iremos criar uma instância de um modelo de árvore de decisão para utilizar como base do RFE.\n",
        "model = DecisionTreeClassifier()\n",
        "\n",
        "# Em seguida, criamos a instância do RFE propriamente dito, utilizando como\n",
        "# modelo base a árvore de decisão. Também definimos que queremos que o processo\n",
        "# seja encerrado quando tivermos encontrado os três atributos de mairo relevância.\n",
        "selector = RFE(estimator=model, n_features_to_select=3)\n",
        "X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "# Após o processamento, podemos criar um dataframe para exibir os resultados do processamento\n",
        "df_rfe = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Selected': selector.support_,  # True para os atributos selecionados\n",
        "    'Ranking': selector.ranking_    # 1 para os mais relevantes\n",
        "}).sort_values(by='Ranking')\n",
        "\n",
        "print(\"Resultados do RFE:\")\n",
        "df_rfe"
      ],
      "metadata": {
        "id": "7hYSOWCn6zd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4-Vetorização de Textos\n",
        "\n",
        "<p align=\"justify\">\n",
        "Métodos de aprendizado de máquina voltados para processamento de linguagem natural requerem a utilização de métodos que permitam a representação numérica de textos e suas relações. Um dos métodos mais comuns para a representação de textos é o <strong>Term Frequency-Inverse Document Frequency</strong> ou TF-IDF, uma técnica que combina duas métricas: a frequência de um termo em um documento (TF) e a relevância desse termo em relação ao conjunto de documentos (IDF). ssa abordagem transforma documentos textuais em vetores numéricos, atribuindo pesos maiores a palavras que aparecem frequentemente em um documento, mas raramente nos outros, ajudando a identificar termos importantes e distintivos.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "O TF-IDF é amplamente utilizado em tarefas de processamento de linguagem natural, como classificação de texto, análise de sentimentos e recuperação de informações. Ele é especialmente útil em conjuntos de dados como artigos de notícias, e-mails (para classificação de spam), resenhas de produtos, ou qualquer coleção de textos onde seja importante destacar as palavras mais representativas de cada documento sem que palavras muito comuns (como \"e\", \"de\", \"o\") dominem a análise.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Este notebook apresenta um exemplo prático da utilização do método TF-IDF para a vetorização de um conjunto de dados com algumas frases em português. O método encontra-se disponível na biblioteca Scikit-Learn através da classe <strong>TfidfVectorizer</strong>, a qual será carregada na célula abaixo. Também carregaremos outros módulos necessários para utilizar o TF-IDF no Scikit-Learn, além da biblioteca Pandas para criar e gerenciar o conjunto de dados de exemplo.\n",
        "</p>"
      ],
      "metadata": {
        "id": "SsUex3Eb7W-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "choQaUS07rIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Iremos trabalhar com conjuntos de dados contido no arquivo <strong>spam.csv</strong>. Este arquivo contém amostras de frases que são classificadas como sendo de mensagens indesejadas (spam) ou não.\n",
        "</p>"
      ],
      "metadata": {
        "id": "zPLSGq5u7t1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza a leitura do conjunto de dados para um Dataframe\n",
        "messages = pd.read_csv('https://raw.githubusercontent.com/irajamuller/data_science/main/dataset/spam.csv')\n",
        "\n",
        "messages.head()"
      ],
      "metadata": {
        "id": "aXB0JXkx8Bg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Neste exemplo iremos trabalhar com o treinamento de um modelo de aprendizado baseado em Random Forest. Faremos isso apenas como exemplo da aplicação do conjunto de dados processado através do TF-IDF. Por isso, a célula a seguir divide os atributos do conjunto de dados entre preditores (message) e alvo (is_spam).\n",
        "</p>"
      ],
      "metadata": {
        "id": "bUoVjEWT8ahj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Divide os atributos entre preditores (X) e alvo (y).\n",
        "X = messages['message']\n",
        "y = messages['is_spam']"
      ],
      "metadata": {
        "id": "3jd1ziQS8g_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A próxima etapa será aplicar o método TF-IDF em si para realizar a análise das strings contidas no conjunto de dados e criar o vetor de características que poderá ser utilizado em algoritmos de aprendizado de máquina.\n",
        "</p>"
      ],
      "metadata": {
        "id": "de0hQPwK8kSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma instância da classe \"TfidfVectorizer\" para codificação do conjunto de dados\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Aplica o processo de análise e codificação sobre o array\n",
        "X_tfidf = vectorizer.fit_transform(X)"
      ],
      "metadata": {
        "id": "K_DTtMXa8tbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Podemos visualizar o resultado do processo de codificação extraindo as informações do resultado da codificação e da instância do codificador. A célula a seguir compila estes resultados e apresenta um sumário com as frequências identificadas para cada uma das palavras presentes no conjunto de dados.\n",
        "</p>"
      ],
      "metadata": {
        "id": "HgnzQ3Fv8zOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extrai as frequências obtidas para cada um dos termos que o codificador identificou durante o processo\n",
        "word_frequencies = X_tfidf.toarray().sum(axis=0)\n",
        "# Extra o array com todos os termos identificados pelo codificador\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Cria um Dataframe combinando as informações de vocabulário e frequência e o ordena pela frequência\n",
        "freq_df = pd.DataFrame({\n",
        "    'Palavra': vocabulary,\n",
        "    'Frequência': word_frequencies\n",
        "})\n",
        "freq_df.sort_values(by='Frequência', ascending=False)"
      ],
      "metadata": {
        "id": "O24wBwVZ84XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Com o processo de codificação realizado, podemos trabalhar com os dados em diferentes algoritmos de aprendizado. A célula abaixo apresenta um exemplo onde utilizamos o conjunto de dados criado para treinar um modelo simples de Random Forest para o reconhecimento de mensagens de SPAM.\n",
        "</p>"
      ],
      "metadata": {
        "id": "Y-jZQANW9BeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iremos criar uma divisão do conjunto de dados em partes para treino e teste para realizar\n",
        "# um processo de treinamento de um modelo de aprendizado de máquina simples.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3)\n",
        "\n",
        "# Treinar um modelo Random Forest com os embeddings\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Fazer previsões e avaliar\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Acurácia do modelo com embeddings TF-IDF: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Z5FZUG8O9FuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5-Análise de Componente Principal\n",
        "\n",
        "<p align=\"justify\">\n",
        "A Análise de Componentes Principais (PCA) é uma técnica estatística utilizada para reduzir a dimensionalidade de conjuntos de dados, transformando variáveis correlacionadas em um novo conjunto de variáveis não correlacionadas chamadas componentes principais. O objetivo do PCA é reduzir a complexidade de conjuntos de dados com muitos atributos, especialmente quando há colinearidade ou redundância entre as variáveis. O PCA alcança isso ao transformar as variáveis originais em um conjunto menor de componentes principais, que mantêm a maior parte da variância dos dados originais.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Um exemplo de um conjunto de dados que pode se beneficiar da redução de dimensionalidade é o dataset <strong>digits</strong>. Este conjunto de dados contém 1797 amostras de dígitos escritos à mão, compostos por imagens em tons de cinza com $8 \\times 8$ pixels. Portanto, o conjunto de dados possui um total de 64 atributos, um para cada píxel da imagem. A existência de tantos atributos pode dificultar tarefas como a visualização de dados ou o tratamento do conjunto de dados por algoritmos de aprendizado.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "O processo de PCA é adequado para estas situações, tendo como objetivo reduzir a dimensionalidade do conjunto de dados sem ocasionar a perda de informações presentes nos atributos. Neste notebook iremos utilizar o método do PCA para reduzir a dimensionalidade do conjunto de dados digits.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Primeiramente iremos carregar as bibliotecas necessárias para manipular o conjunto de dados e realizar o processo de PCA. Também iremos carregar o Matplotlib para realizar uma análise visual do conjunto de dados.\n",
        "</p>"
      ],
      "metadata": {
        "id": "VaNrlPc19y6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "upxLPQsf-DCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A célula a seguir realiza o carregamento do conjunto de dados digits através da biblioteca Scikit-Learn. Em seguida, aplica-se o método <strong>train_test_split</strong> para dividir o conjunto de dados em partições para treino e teste. Esta divisão de dados é comumente realizada quando trabalhamos com métodos de aprendizado de máquina. No caso do exemplo, a divisão irá destinar 30% do conjunto de dados para testar o modelo de aprendizado criado com os 70% restante do conjunto.\n",
        "</p>"
      ],
      "metadata": {
        "id": "Pl7QtVvK-GqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Carregar o dataset Digits do sklearn\n",
        "data = load_digits()\n",
        "X, y = data.data, data.target # data é a imagem e target é o dígito\n",
        "\n",
        "# Divisão treino/teste\n",
        "# X é o conjunto de dados com as features\n",
        "# Y é o vetor com os labels ou variávei alvo (o que se deseja prever)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "NK0V7vtU-S67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir as primeiras 10 imagens com seus rótulos\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(data.images[i], cmap='gray')  # digits.images já tem o formato 8x8\n",
        "    ax.set_title(f\"Label: {digits.target[i]}\")\n",
        "    ax.axis('off')  # tirar os eixos para ficar mais limpo\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GtL-sd_QCehm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A célula abaixo cria uma instância da classe PCA disponível no Scikit-Learn. O parâmetro <strong>n_components</strong> define o número de componentes que será criado para o novo dataset. Nosso exemplo transformará os 64 atributos originais em 2 atributos, buscando manter o máximo de informação estatística possível sobre o conjunto nestes atributos. É possível especificar um número maior de atributos, permitindo reter ainda mais informações sobre o conjunto original. Entretanto, um maior número de atributos também irá resultar em maior complexidade computacional quando aplicado a algoritmos de aprendizado.\n",
        "</p>"
      ],
      "metadata": {
        "id": "3ljHcbyH-WJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Análise de Componentes Principais (PCA)\n",
        "# Reduzir para 2 componentes principais para visualização\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "metadata": {
        "id": "dziscEe3-fpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Quando aplicamos o método PCA, cada um dos atributos criados reterá uma parte da informação sobre a variância existente no conjunto de dados original. Podemos verificar o total de variância que foi preservada pelo processo de PCA utilizando o método <strong>explained_variance_ratio_</strong>. Ele apresentará um vetor com o total de variância preservado em cada um dos atributos. Conforme já mencionado, quanto maior o número de atributos, maior será a variância preservada do conjunto original.\n",
        "</p>"
      ],
      "metadata": {
        "id": "9750cLpT-mff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar a variação explicada\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "print(f\"Variância explicada pelos componentes principais: {explained_variance}\")\n",
        "print(\"Variância explicada por 2 componentes:\", sum(pca.explained_variance_ratio_))"
      ],
      "metadata": {
        "id": "CV45hM0z-5Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Agora que projetamos os 64 atributos em dois componentes principais, podemos visualizá-lo na forma de um gráfico 2D. A célula abaixo cria uma visualização baseada em um gráfico de dispersão que apresenta o conjunto de dados após a projeção em dois componentes principais.\n",
        "</p>"
      ],
      "metadata": {
        "id": "KXgAsgoU--gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualização dos dados reduzidos\n",
        "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(label='Classe')\n",
        "plt.xlabel('Componente Principal 1')\n",
        "plt.ylabel('Componente Principal 2')\n",
        "plt.title('Dados projetados em 2 Componentes Principais')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9jU1difo_Di_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Por fim, podemos utilizar o conjunto de dados processador para realizar o treinametno de um modelo de aprendizado simples a fim de verificar a acurácia resultante. No exemplo abaixo, iremos treinar um modelo de <strong>Random Forest</strong> e verificar sua acurácia utilizando a divisão em conjuntos de treino e teste realizada anteriormente.\n",
        "</p>"
      ],
      "metadata": {
        "id": "zeKnNcRf_L5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinar um modelo Random Forest com dados reduzidos\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train_pca, y_train)\n",
        "\n",
        "# Fazer previsões e avaliar\n",
        "y_pred = model.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Acurácia do modelo com dados reduzidos pelo PCA: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "P2B5m-lJ_T0U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}