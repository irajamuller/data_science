{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPzBZX2IDk34vVF4dbJXsSM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/irajamuller/data_science/blob/main/Prepara%C3%A7%C3%A3o_de_Dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.Tratamento Básico de Dados**\n",
        "---\n",
        "<p align=\"justify\">\n",
        "Este notebook apresenta os métodos mais fundamentais para o tratamento de conjuntos de dados utilizando o Dataframe da biblioteca Pandas. Iremos nos focar nos métodos para trabalhar com:\n",
        "</p>\n",
        "\n",
        "- Ajuste de colunas;\n",
        "- Análise e tratamento de dados faltantes;\n",
        "- Análise e tratamento de dados duplicados.\n",
        "\n",
        "<p align=\"justify\">\n",
        "Para esta atividade iremos utilizar uma versão modificada do conjunto de dados <strong>titanic</strong>. A versão original do conjunto de dados encontra-se disponível através da biblioteca Seaborn. Entretanto, neste exercício, iremos utilizar uma versão que apresenta dados modificados para ressaltar alguns problemas presentes em conjuntos de dados não tratados.\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "Neste notebook iremos utilizar apenas a biblioteca Pandas. Primeiramente, iremos carregar o conjunto de dados modificado, o qual encontra-se disponível no github <strong>titanic.csv</strong>.\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "3YSjVf-dlsl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/irajamuller/data_science/main/dataset/titanic.csv') # Carrega o conjunto de dados\n",
        "df.info() # Apresenta informações gerais sobre a estrutura do conjunto de dados"
      ],
      "metadata": {
        "id": "ttiEwFpdx3io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head() # Exibe as primeiras entradas do conjunto de dados"
      ],
      "metadata": {
        "id": "9TxLaMnzh5St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "É importante compreender o contexto e o significado de cada um dos atributos do conjunto de dados para saber como utilizá-lo em tarefas de aprendizado de máquina. O <strong>titanic</strong> é um conjunto de dados aberto e bem documentado, permitindo facilmente encontrar informações sobre o significado e características de cada um dos atributos.\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "Vamos olhar primeiramente para os nomes das colunas. Algumas colunas apresentam nomes abreviados. Por exemplo, \"sibsp\" é um valor inteiro que indica quantos irmãos (siblings) ou cônjuges (spouses) estavam à bordo junto com o passageiro. Da mesma forma \"parch\" é um valor inteiro que indica quantos pais (parents) ou filhos (children) estavam à bordo junto com o passageiro. Para facilitar nossa interpretação do conjunto de dados, podemos renomear estas colunas. O código abaixo apresenta este processo.\n",
        "</p>\n",
        "<div align=\"center\">\n",
        "\n",
        "| **Coluna**    | **Descrição**                                                 |\n",
        "| ------------- | ------------------------------------------------------------- |\n",
        "| `survived`    | Sobreviveu (1 = sim, 0 = não)                                 |\n",
        "| `pclass`      | Classe do bilhete (1ª, 2ª, 3ª)                                |\n",
        "| `sex`         | Sexo do passageiro (`male` ou `female`)                       |\n",
        "| `age`         | Idade do passageiro                                           |\n",
        "| `sibsp`       | Nº de irmãos/cônjuges a bordo                                 |\n",
        "| `parch`       | Nº de pais/filhos a bordo                                     |\n",
        "| `fare`        | Valor pago na passagem (em libras)                            |\n",
        "| `embarked`    | Porto de embarque (`C`, `Q`, `S`)                             |\n",
        "| `class`       | Classe do bilhete (`First`, `Second`, `Third`)                |\n",
        "| `who`         | Tipo de pessoa (`man`, `woman`, `child`)                      |\n",
        "| `adult_male`  | Se é um homem adulto (`True` ou `False`)                      |\n",
        "| `deck`        | Letra do deck da cabine (`A` a `G`, ou ausente)               |\n",
        "| `embark_town` | Cidade de embarque (`Cherbourg`, `Queenstown`, `Southampton`) |\n",
        "| `alive`       | Está vivo (`yes`) ou não (`no`)                               |\n",
        "| `alone`       | Estava sozinho a bordo (`True` ou `False`)                    |\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "8qYYEuqQyTON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Renomear colunas\n",
        "df = df.rename(columns={\n",
        "    \"sibsp\": \"siblings_spouses\",\n",
        "    \"parch\": \"parent_children\",\n",
        "    \"sex\": \"gender\"\n",
        "})\n",
        "\n",
        "df.info() # Exibe os metadados do conjunto de dados"
      ],
      "metadata": {
        "id": "GKpoNrsfybzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Valores Ausentes**\n",
        "<p align=\"justify\">\n",
        "Nosso próximo passo é avaliar os valores ausentes no conjunto de dados. Valores ausentes se referem a entradas no conjunto de dados que não possuem um valor registrado em um ou mais de seus atributos. Ao exibir o conjunto de dados, o Pandas mostra estas entradas através de um <strong>NaN</strong>, que é a abreviação para <strong>Not a Number</strong>. De modo geral, existem dois motivos porque registros podem apresentar atributos com valor ausente:\n",
        "</p>\n",
        "\n",
        "- A ausência do valor está correta pelo fato de que o valor de fato não existe;\n",
        "- Houve um problema na coleta de dados que impediu a aquisição do valor para uma parcela dos registros.\n",
        "\n",
        "<p align=\"justify\">\n",
        "Um exemplo do primeiro caso seria a existência de uma coluna para nome do cônjuge, a qual deveria permanecer vazia para passageiros solteiros. Neste caso, faz sentido não haver um valor registrado neste item e ele deve ser considerado pelo processo de aprendizado. Por sua vez, o segundo caso pode ocorrer por motivos como um processo de coleta falho ou erros de leitura em dispositivos. Nestes casos, deveria haver um valor nesta coluna, mas os motivos acima impediram que ele fosse registrado.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Existem diferentes métodos que podemos utilizar para tratar valores ausentes. O método a ser utilizado vai depender de diferentes fatores como a porcentagem de valores ausentes e a significância do parâmetro para o processo de aprendizado.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "A célula abaixo demonstra como podemos avaliar o número de registros com valores ausentes para cada um dos atributos do conjunto de dados.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "FEWatg7gymjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Apresenta, para cada coluna, o total de registros que possui valores vazios.\n",
        "Também podem ser visualizado através do método \"info\".\n",
        "'''\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "V7D4SNm3ypQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O método mais simples de tratamento é a remoção de qualquer registro que possua algum atributo com valor ausente no conjunto de dados. A célula abaixo apresenta o código para realizar este tipo de tratamento.\n",
        "</p>"
      ],
      "metadata": {
        "id": "brZ8gA3Bywdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropped = df.dropna() # Realiza a remoção de qualquer registro no dataframe que possua algum valor vazio.\n",
        "df_dropped.info() # O resultado abaixo mostra que a remoção das colunas resultou na eliminação de 79,6% do Conjunto de dados."
      ],
      "metadata": {
        "id": "30wWa5TUy1XG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Como podemos ver, a simples remoção de todos os registros com algum valor ausente causou a eliminação de aproximadamente 80% do conjunto de dados, o que pode inviabilizar a utilização do conjunto de dados. Uma alternativa mais interessante pode ser remover atributos que apresentam maior ausência de dados e não são relevantes para o processo de aprendizado. Os metadados do conjunto mostram que o atributo <strong>deck</strong> apresenta a maior parte dos valores ausentes. Portanto, iremos eliminá-lo de nosso conjunto de dados antes de remover os valores ausentes.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "SVRV32pwy8nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove uma coluna do conjunto de dados.\n",
        "df_dropped = df.drop(columns=['deck'])\n",
        "# Repetimos a operação de limpeza dos demais registros que possuem valores em branco.\n",
        "df_dropped = df_dropped.dropna()\n",
        "\n",
        "'''\n",
        "A remoção da coluna \"deck\" evita que uma porção considerável do conjunto de dados seja descartada\n",
        "Como resultado, o conjunto de dados limpo perdeu apenas 20,1% em comparação ao original.\n",
        "'''\n",
        "df_dropped.info()\n"
      ],
      "metadata": {
        "id": "bUCNukgnzBsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Dependendo do conjunto de dados e suas características, também podemos preencher os valores faltantes com dados sintéticos, tais como valores gerados a partir de cálculos estatísticos. Este tipo de tratamento requer uma análise cuidadosa dos valores gerados e suas implicações para o restante dos dados, pois ela pode causar viéses indesejados no processo de aprendizado.\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "Analisando nosso conjunto de dados, podemos ver que a coluna <strong>age</strong>, que registra a idade do passageiro, possui o segundo maior número de valores ausentes. Uma possível solução para eliminar os valores faltantes nesta coluna sem perder os registros através de eliminação seria preencher a idade dos passageiros com valores gerados a partir da análise do restante dos dados no conjunto.\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "No exemplo abaixo iremos realizar um preenchimento simples utilizando o valor da mediana da idade dos passageiros.\n",
        "</p>"
      ],
      "metadata": {
        "id": "iFqsUaqdnE86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_dropped = df.drop(columns=['deck']) # Remove uma coluna do conjunto de dados.\n",
        "'''\n",
        "Ao invés de remover os registros com valores faltantes, podemos substituílos por um valor padrão, por exemplo zero.\n",
        "No exemplo abaixo substituímos os valores faltantes apenas da coluna \"age\" por zero.\n",
        "'''\n",
        "median_age = df_dropped['age'].median()\n",
        "df_dropped['age'] = df_dropped['age'].fillna(median_age)\n",
        "\n",
        "df_dropped = df_dropped.dropna() # Repetimos a operação de limpeza dos demais registros que possuem valores em branco.\n",
        "df_dropped.info() # O resultado agora mostra que praticamente todo o dataset possui valores completos em seus registros.\n"
      ],
      "metadata": {
        "id": "xA1Y3Rae0XAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Além de valores faltantes, outra questão que requer atenção é a existência de <strong>registros duplicados</strong> no conjunto de dados. Dependendo do contexto do conjunto de dados e sua finalidade, registros duplicados podem causar inconsistências como o desbalanceamento de classes, o que poderia resultar e viéses indesejaveis em modelos de aprendizado. Por esse motivo, o Pandas oferece métodos que permitem identificar e eliminar registros duplicados.\n",
        "\n",
        "As células abaixo apresentam os métodos para identificar o número de registros duplicados e, após, realizar a sua eliminação do conjunto de dados.\n",
        "</p>"
      ],
      "metadata": {
        "id": "2arLaBsooCig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar itens duplicados (em todo o dataset)\n",
        "df_duplicated = df[df.duplicated()]\n",
        "df_duplicated.info()"
      ],
      "metadata": {
        "id": "h3ABPf6BphLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar itens duplicados\n",
        "# Cria uma lista que indica para cada registro do dataset se ele é uma duplicata ou não através de um valor booleano.\n",
        "df_duplicated = df_dropped.duplicated()\n",
        "print( f\"Registros duplicados: {df_duplicated.sum()}\" )\n"
      ],
      "metadata": {
        "id": "hUOGqiNX0ugU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Realiza a limpeza de todas as duplicatas encontradas.\n",
        "df_dropped = df_dropped.drop_duplicates()"
      ],
      "metadata": {
        "id": "xMXWoXb0qSR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2.Padronização de Dados\n",
        "<p align=\"justify\">\n",
        "A padronização de dados diz respeito a diferentes tarefas de conversão que têm por objetivo tornar os dados tratáveis por métodos de aprendizado de máquina. É comum que conjuntos de dados reais apresentem inconsistências de preenchimento em dados categóricos ou de data, sendo necessário realizar tratamentos para que eles sejam interpretados de forma correta. Além disso, algoritmos de aprendizado de máquina apresentam melhor acurácia quando dados numéricos são normalizados para valores dentro de uma escala padronizada. Devemos analisar o nosso conjunto de dados e escolher os métodos que devem ser aplicados dependendo de quais algoritmos de aprendizado serão utilizados posteriormente.\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "Iremos trabalhar com métodos da biblioteca Pandas para a realização das seguintes atividades:\n",
        "</p>\n",
        "\n",
        "- Conversão de dados categóricos;\n",
        "- Tratamento de formatos de data;\n",
        "- Normalização de valores numéricos.\n",
        "\n",
        "<p align=\"justify\">\n",
        "Nesta atividade iremos utilizar um conjunto de dados <strong>fictício</strong> que apresenta informações sobre salários e tempo de serviço de pessoas em uma empresa. Este conjunto de dados encontra-se disponível no arquivo <strong>people.csv</strong>. Primeiramente iremos carregar a biblioteca Pandas, a qual iremos utilizar no restante da atividade, além de realizar a carga do conjunto de dados. Também iremos tratar os dados ausentes na coluna <strong>salario_bruto</strong> utilizando a mediana dos demais valores deste atributo.\n",
        "</p>"
      ],
      "metadata": {
        "id": "U9lIlEe6ANE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/irajamuller/data_science/main/dataset/people.csv') # Carregar o conjunto de dados\n",
        "df\n"
      ],
      "metadata": {
        "id": "nJGsB0UPAuzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mediana_salario = df[\"salario_bruto\"].median() # Tratar os dados ausentes na coluna de salários com a mediana dos valores presentes\n",
        "df[\"salario_bruto\"] = df[\"salario_bruto\"].fillna(mediana_salario).astype(float)"
      ],
      "metadata": {
        "id": "IbdYmh11Bb1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "Fm1Ci2OlBUmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Primeiramente iremos padronizar o formato de colunas com dados de string. Em vários casos, dados de string são utilizados para categorização dos demais atributos. Apenas lembrando, atributos categóricos permitem classificar os dados entre diferentes categorias predefinidas. No conjunto de dados que estamos trabalhando, um exemplo é o atributo <strong>estado</strong>, que indica o estado de origem do empregado. Observando os dados contidos na coluna, podemos ver que há tanto versões onde o estado está em maiúsculas e, em outros casos, em minúsculas. Estas diferenças de formatação podem causar inconsistências quando tentamos converter essa coluna para um tipo categórico, resultando em mais de uma categoria criada para um mesmo estado, por exemplo.\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "Neste caso, precisamos tratar estes atributos para padronizar o seu formato. Como exemplo, iremos converter a coluna <strong>estado</strong> para um formato de caixa alta (todos caracteres maiúsculos) e a coluna <strong>cargo</strong> para um formato de caixa baixa (todos caracteres minúsculos).\n",
        "</p>"
      ],
      "metadata": {
        "id": "_h2yDD54B6WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"estado\"] = df[\"estado\"].str.upper() # Padronizar strings de estado para uppercase\n",
        "df[\"cargo\"] = df[\"cargo\"].str.lower().str.strip() # Padronizar cargos para lowercase e remover espaços extras\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "EFMP_57GCS2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Após a padronização destas informações, podemos transformar a coluna estado em um atributo categórico, conforme apresentado na célula abaixo.\n",
        "</p>"
      ],
      "metadata": {
        "id": "YpaT4MO_C6wR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"estado\"] = df[\"estado\"].astype(\"category\") # Converter estado para categoria\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "BsiDpcszC5C2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Em seguida, iremos tratar os dados de data que estão presentes nas colunas <strong>data_nascimento</strong> e <strong>ultima_avaliacao</strong>. Por padrão, o Pandas armazena a informação de datas como strings, sendo necessário converter estes atributos para informações de data e hora de forma explícita. Partindo do princípio que as colunas contenham formatos padrão para o armzenamento de informações de data e hora, a conversão pode ser realizada através do método <strong>to_datetime</strong>, conforme apresentado a seguir.\n",
        "</p>"
      ],
      "metadata": {
        "id": "s_Toj6IjDvXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Corrigir tipos de dados\n",
        "df[\"data_nascimento\"] = pd.to_datetime(df[\"data_nascimento\"])\n",
        "df[\"ultima_avaliacao\"] = pd.to_datetime(df[\"ultima_avaliacao\"])\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "1PkExhzWDsbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Podemos realizar processamentos adicionais com informações de data e hora. Por exemplo, podemos criar atributos adicionais com elementos individuais como o ano e o mês. Desta forma, o algoritmo de aprendizado pode tratar essas informações de forma individual caso isso seja necessário para a análise a ser realizada. A célula abaixo apresenta como é possível criar dois novos atributos a partir da informação contida na coluna <strong>ultima_avaliacao</strong>, a qual já foi previamente convertida para uma coluna com dados temporais.\n",
        "</p>"
      ],
      "metadata": {
        "id": "roI4tbAUEHEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar colunas derivadas a partir de datas\n",
        "df[\"ano_ultima_avaliacao\"] = df[\"ultima_avaliacao\"].dt.year\n",
        "df[\"mes_ultima_avaliacao\"] = df[\"ultima_avaliacao\"].dt.month"
      ],
      "metadata": {
        "id": "Kebp7BHbEQ3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Uma tarefa comum no tratamento de conjuntos de dados é a aplicação de técnicas para a normalização de valores de atributos. O objetivo da normalização é padronizar os valores de um atributo para uma escala controlada. O processo de normalização é comumente utilizado para escalar os valores de atributos para uma faixa aceita pelos algoritmos de aprendizado de máquina que serão aplicados. De forma geral, algoritmos de aprendizado apresentam melhores resultados quando não há uma disparidade muito grande entre as escalas de valores utilizadas pelos diferentes atributos numéricos. Além disso, algoritmos que trabalham com o conceito de distância, tal como clusterização, requerem que os atributos encontrem-se em faixas determinadas, por exemplo entre valores 0 e 1.\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "Existem diferentes métodos que podem ser aplicados para normalizar os valores de atributos. Neste notebook iremos trabalhar com o método de normalização min-max, que pode ser diretamente implementado através dos métodos disponíveis na biblioteca Pandas. Mais adiante também iremos trabalhar com outros métodos de normalização que podem ser implementados utilizando outras bibliotecas do Python como o SciPy.\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "Abaixo temos a fórmula utilizada para a normalização de valores utilizando min-max.\n",
        "</p>\n",
        "$X' = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$\n",
        "\n",
        "- $X$ é o valor original que queremos normalizar\n",
        "- $X_{\\text{min}}$ é o valor mínimo presente no atributo que será normalizado\n",
        "- $X_{\\text{max}}$ é o valor máximo presente no atributo que será normalizado\n",
        "- $X'$ é o valor obtido a partir do processo de normalização\n",
        "\n",
        "<p align=\"justify\">\n",
        "A célula abaixo apresenta a aplicação deste método para dois atributos numéricos presentes no conjunto de dados.\n",
        "</p>"
      ],
      "metadata": {
        "id": "GmnQ93RkEd8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizar salário utilizando escalonamento Min-Max\n",
        "df[\"salario_normalizado\"] = (df[\"salario_bruto\"] - df[\"salario_bruto\"].min()) / (df[\"salario_bruto\"].max() - df[\"salario_bruto\"].min())\n",
        "\n",
        "# Normalizar tempo de empresa utilizando escalonamento Min-Max\n",
        "df[\"tempo_empresa_anos\"] = (df[\"tempo_empresa_anos\"] - df[\"tempo_empresa_anos\"].min()) / (df[\"tempo_empresa_anos\"].max() - df[\"tempo_empresa_anos\"].min())\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ahPuhkPzEvAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.Agregação de Dados\n",
        "<p align=\"justify\">\n",
        "Métodos de agregação de dados permitem sintetizar grandes volumes de informação e reduzir a complexidade computacional em seu gerenciamento. Métodos de agregação usualmente buscam a análise das tendências centrais de conjuntos de dados agrupados de acordo com categorias ou intervalos temporais. Estas técnicas são úteis para casos como conjuntos de dados que trabalham com dados temporais de alta granularidade, os quais usualmente apresentam um grande volume de dados que pode reduzir a eficiência de métodos de aprendizado de máquina complexos. Nestes casos, o uso da agregação permite reduzir a dimensionalidade do conjunto de dados sem comprometer a sua acurácia.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Métodos de agregação usualmente envolvem cálculos estatísticos de tendência central, como análise de médias, medianas e desvio padrão. Estes métodos estão disponíveis nativamente na biblioteca Pandas, mas também podem ser cálculados utilizando outras ferramentas do Python, como veremos mais adiante.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Este notebook apresenta alguns exemplos da aplicação de métodos de agregação utilizando o conjunto de dados disponível no arquivo <strong>people.csv</strong>, o qual já utilizamos neste módulo. Primeiramente iremos carregar a biblioteca Pandas e o respectivo conjunto de dados. Também iremos fazer uso de um método para padronizar a informação contida na coluna <strong>estado</strong> para caixa alta.</p>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "0ZX3XOO-Y1mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/irajamuller/data_science/main/dataset/people.csv') # Carrega o conjunto de dados\n",
        "\n",
        "df[\"estado\"] = df[\"estado\"].str.upper().astype(\"category\") # Padroniza os valores da coluna \"estado\" para caixa alta.\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "GP1M5R9NZNlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A célula abaixo apresenta uma agregação simples onde os dados do atributo <strong>salario_bruto</strong> são agregados em função das categorias presentes no atributo <strong>estado</strong>. Para cada categoria, os valores de <strong>salario_bruto</strong> são agregado utilizando a média aritmética simples através do método <strong>mean</strong>.\n",
        "</p>"
      ],
      "metadata": {
        "id": "d9mV1JuNZp80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Média de salário por ano\n",
        "df.groupby(\"estado\", observed=True)[\"salario_bruto\"].mean()"
      ],
      "metadata": {
        "id": "j9k9yNU_Z4N6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O exemplo abaixo adiciona o ordenamento decrescente à exibição dos valores agregados por estado.\n",
        "</p>"
      ],
      "metadata": {
        "id": "0ZD-8eAIa-Rk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ordenar estados pela média de salário, em ordem decrescente\n",
        "df.groupby(\"estado\", observed=True)[\"salario_bruto\"].mean().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "BLDL-9UrbDsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Um outro exemplo básico de agregação é a contagem de elementos presentes em uma categoria. A célula abaixo apresenta uma agregação onde cada registro contém o número de funcionários categorizado pelo atributo `estado`.\n",
        "</p>"
      ],
      "metadata": {
        "id": "eY-NVNSebb7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Contar o número de funcionários por estado\n",
        "df.groupby(\"estado\", observed=True)[\"id\"].count()"
      ],
      "metadata": {
        "id": "foiLS7qVba7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "No próximo exemplo iremos utilizar o atributo <strong>estado</strong> para agregar os valores de salário, similar ao primeiro exemplo. Porém, dessa vez iremos aplicar o método <strong>agg</strong>, que permite o cálculo de múltiplas métricas de tendência central de forma simultânea. A célula abaixo apresenta um exemplo com o número de entradas e valores de salário mínimo, máximo, médio e mediano.\n",
        "</p>"
      ],
      "metadata": {
        "id": "nHIrpXDib7EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Obter o salário mínimo, máximo e médio por cargo\n",
        "df.groupby(\"estado\", observed=True)[\"salario_bruto\"].agg([\"count\", \"min\", \"max\", \"mean\", \"median\"])"
      ],
      "metadata": {
        "id": "01j5UtFQcJW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O método <strong>agg</strong> também permite o uso de funções personalizadas para a o cálculo de agregações além daquelas nativamente oferecidas. No exemplo abaixo, definimos uma função <strong>range_func</strong> que calcula a faixa entre os salários mínimos e máximos para cada item da agregação. Em seguida, a utilizamos no método <strong>agg</strong> para obter a faixa salarial para cada uma das categorias por estado.\n",
        "</p>"
      ],
      "metadata": {
        "id": "QGlYf6MOcSEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir uma função personalizada\n",
        "def my_min_max(x):\n",
        "    return x.max() - x.min()\n",
        "\n",
        "# Aplicar a função personalizada\n",
        "df.groupby(\"estado\", observed=True)[\"salario_bruto\"].agg([\"mean\", \"median\", my_min_max])"
      ],
      "metadata": {
        "id": "jjuWhMKacj51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O método <strong>groupby</strong> também permite a agregação por múltiplos atributos. No exemplo abaixo, realiza-se a agregação do valor de salários de acordo com o estado e o cargo dos funcionários.\n",
        "</p>"
      ],
      "metadata": {
        "id": "9U0b1HIofCnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar por estado e cargo e calcular a média do salário\n",
        "df.groupby([\"estado\", \"cargo\"], observed=True)[\"salario_bruto\"].mean()"
      ],
      "metadata": {
        "id": "cXaqpkaWfKIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A agregação também pode ser utilizada para a criação de novos atributos no conjunto de dados, acrescentando informações a cada um dos registros disponíveis. No exemplo abaixo, será adicionada o atributo <strong>media_salario_estado</strong>, contendo o valor médio do salário agregado pelo estado ao qual o funcionário pertence. Observe que o valor é adicionado para todos os registros do conjunto de dados de acordo com o valor da coluna <strong>estado</strong>.\n",
        "</p>"
      ],
      "metadata": {
        "id": "qinHDXJAfPdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar uma coluna com a média do salário por estado\n",
        "df[\"media_salario_estado\"] = df.groupby(\"estado\", observed=True)[\"salario_bruto\"].transform(\"mean\")\n",
        "df"
      ],
      "metadata": {
        "id": "dKDhI5wQfY0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A combinação dos métodos <strong>groupby</strong> e <strong>agg</strong> também pode ser utilizada para o cálculo de valores agregados em múltiplos atributos. No exemplo abaixo, o atributo <strong>estado</strong> é utilizado para o agrupamento de valores de ambos atributos <strong>salario_bruto</strong> e <strong>tempo_empresa_anos</strong>. Observe que, para cada atributo, diferentes valores agregados serão calculados.\n",
        "</p>"
      ],
      "metadata": {
        "id": "JUnh6-hmgxA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"estado\", observed=True).agg({\n",
        "    \"salario_bruto\": [\"mean\", \"sum\"],\n",
        "    \"tempo_empresa_anos\": [\"max\", \"min\"]\n",
        "})"
      ],
      "metadata": {
        "id": "hLECA6v5g_kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Métodos de agrupamento também podem ser aplicados a atributos numéricos, em particular para a definição de faixas de valores. A célula abaixo apresenta um exemplo onde os valores de salário são agrupados de acordo com a faixa etária dos empregados. Para isso, definem-se três faixas etárias com diferentes escalas de idades. Em seguida, as faixas etárias são utilizadas para calcular a média salarial para os registros presentes em cada faixa etária.\n",
        "</p>"
      ],
      "metadata": {
        "id": "110_9YUbhOZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agrupar por faixas etárias [18, 30), [30, 50), [50, 80)\n",
        "df[\"faixa_etaria\"] = pd.cut(df[\"idade\"], bins=[18, 30, 50, 80], labels=[\"Jovem\", \"Adulto\", \"Idoso\"])\n",
        "df.groupby(\"faixa_etaria\", observed=True)[\"salario_bruto\"].mean()"
      ],
      "metadata": {
        "id": "PnwEKCGthUrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.Tratamento de Outliers\n",
        "<p align=\"justify\">\n",
        "<strong>Outliers</strong> são valores em um conjunto de dados que diferem significativamente do restante dos valores observados. Existem vários motivos para a ocorrência de outliers, os quais podem incluir erros de leitura ou entrada de dados durante o processo de coleta ou fenômenos raros que saem do padrão esperado para o restante dos dados. A existência de outliers pode levar a resultados indesejáveis em processos de aprendizado de máquina como a perda de aucrácia devido à existência de valores que não apresentam o comportamento normal esperado para o conjunto de dados. Por isso, dependendo do objetivo esperado para o processo de aprendizado, torna-se necessário identificar e tratar a existência de outliers no conjunto de dados.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "A identificação de outliers requer a aplicação de métodos estatísticos para a identificação das tendências centrais dos atributos do conjunto de dados e, posteriormente, a remoção dos valores identificados como fora do padrão. Neste notebook iremos trabalhar com o método de análise de intervalo entre quartis (ou IQR do inglês). Este método busca identificar valores em um atributo que se encontrem fora de um determinado limiar além do intervalo entre quartis da distribuição de dados para um atributo. Usualmente, outliers são definidos como valores que se encontram:\n",
        "</p>\n",
        "\n",
        "- Abaixo de $Q1 - 1,5 \\times \\text{IQR}$\n",
        "- Acima de $Q3 + 1,5 \\times \\text{IQR}$\n",
        "\n",
        "<p align=\"justify\">\n",
        "Onde Q1 e Q3 representam, respectivamente, os quartis de 25% e 75% da distribuição de dados. O valor 1,5 é uma constante que define o limiar que define se um valor é um outlier ou não. Esta constante é um valor amplamente utilizado na aplicação do método IQR.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Neste notebook iremos trabalhar com a identificação de outliers nos atributos do conjunto de dados disponível no arquivo <strong>titanic.csv</strong>, o qual já utilizamos para outras atividades neste módulo. Primeiramente iremos carregar a biblioteca Pandas e, em seguida, o conjunto de dados que iremos utilizar.\n",
        "</p>"
      ],
      "metadata": {
        "id": "ynQazgKYndrb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/irajamuller/data_science/main/dataset/titanic.csv')\n",
        "df.info()"
      ],
      "metadata": {
        "id": "D98NSWsdoNu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A biblioteca Pandas não apresenta uma função específica para a aplicação do método IQR e identificação dos outliers. Entretanto, podemos criar uma função do Python que implementa este método de acordo com o que já estudamos sobre o seu funcionamento. A célula abaixo define a função que implementa o método IQR.\n",
        "</p>"
      ],
      "metadata": {
        "id": "ATKc8xrlobZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Método que implementa a identificação de valores considerados outliers a partir do método IQR.\n",
        "Recebe como parâmetro uma Series do Pandas contendo o atributo numérico que será analisado\n",
        "e retorna uma Series contendo todos os outliers identificados pelo método.\n",
        "'''\n",
        "def buscar_outliers(serie_coluna: pd.Series) -> pd.Series:\n",
        "    Q1 = serie_coluna.quantile(0.25)\n",
        "    Q3 = serie_coluna.quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = serie_coluna[ (serie_coluna < Q1 - 1.5 * IQR) | (serie_coluna > Q3 + 1.5 * IQR) ]\n",
        "    return outliers"
      ],
      "metadata": {
        "id": "YVnuwyjrouMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O método acima é genérico e pode ser aplicado a qualquer Series do Pandas que possua valores numéricos. Com o método definido, podemos utilizá-lo para identificar os outliers presentes nos diferentes atributos do nosso conjunto de dados. A célula abaixo apresenta a aplicação do método para identificar os outliers presentes no atributo <strong>fare</strong>.\n",
        "</p>"
      ],
      "metadata": {
        "id": "VzOpvJqio4Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplica o método buscar_outliers para encontrar os outliers no atributo \"fare\"\n",
        "outliers = buscar_outliers(df['fare'])\n",
        "\n",
        "# Imprime o número de outliers identificados e lista os cinco primeiros valores considerados outliers.\n",
        "print(f\"Outliers identificados: {len(outliers)}\")\n",
        "outliers.head()"
      ],
      "metadata": {
        "id": "JRLTvF4MpAt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Após identificar os outliers podemos removê-los do nosso conjunto de dados utilizando o método drop do\n",
        "Dataframe em conjunto com a Series de outliers obtida na célula anterior.\n",
        "'''\n",
        "df_sem_outliers = df.drop(index=outliers.index)\n",
        "df_sem_outliers.info()"
      ],
      "metadata": {
        "id": "0Gtq7gZMpHeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Caso as linhas/tuplas sejam fundamentais e não podem ser removidas, uma alternativa é ajustar o valor outlier substituindo-o pela média ou mediana.\n",
        "</p>"
      ],
      "metadata": {
        "id": "D1cAk-Jdp72p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Q1, Q3 = df['fare'].quantile(0.25), df['fare'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "limite_inferior, limite_superior = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "\n",
        "mediana = df['fare'].median()\n",
        "df['fare_adjusted'] = df['fare'].apply(\n",
        "    lambda x: mediana if x < limite_inferior or x > limite_superior else x\n",
        ")\n",
        "df"
      ],
      "metadata": {
        "id": "YYfhaiU4qVxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.Tratamento de Outliers com SciPy\n",
        "<p align=\"justify\">\n",
        "A SciPy é uma biblioteca de código aberto em Python amplamente utilizada para cálculos científicos e técnicos. O nome vem de Scientific Python. Ela é construída sobre o NumPy e fornece uma grande variedade de funções eficientes e fáceis de usar para tarefas matemáticas e científicas avançadas.\n",
        "</p>\n",
        "\n",
        "Principais funcionalidades da SciPy:\n",
        "- Álgebra linear avançada (scipy.linalg)\n",
        "- Otimização (scipy.optimize)\n",
        "- Integração numérica (scipy.integrate)\n",
        "- Interpolação (scipy.interpolate)\n",
        "- Transformadas de Fourier (scipy.fft)\n",
        "- Estatística (scipy.stats)\n",
        "- Processamento de sinais (scipy.signal)\n",
        "- Processamento de imagens (scipy.ndimage)\n",
        "- Resolução de equações diferenciais (scipy.integrate.solve_ivp)\n",
        "\n",
        "<p align=\"justify\">\n",
        "Este notebook apresenta exemplos de uso da biblioteca SciPy em processos de preparação de conjuntos de dados. O SciPy é uma biblioteca Python de código aberto que fornece ferramentas e algoritmos avançados para cálculos científicos e de engenharia, incluindo otimização, integração, álgebra linear, estatística e processamento de sinais. O SciPy utiliza a biblioteca NumPy como base, utilizando o ndarray como base para a maioria de suas operações. Além disso, ele utiliza implementações em linguagens de alto desempenho para acelerar métodos com maior complexidade computacional.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Além de serem compatíveis com o ndarray do NumPy, vários dos métodos disponíveis no SciPy também são compatíveis com as estruturas da biblioteca Pandas. Tal compatibilidade torna possível a combinação destas diferentes bibliotecas para a implementação de diferentes técnicas para preparação e análise de dados.\n",
        "</p>\n",
        "\n",
        "<p align=\"justify\">\n",
        "Iremos explorar alguns exemplos de análise de dados utilizando os métodos dispníveis nas bibliotecas NumPy e SciPy. Além destas bibliotecas, iremos carregar as bibliotecas Pandas e Seaborn, que serão utilizadas para carregar o dataset <strong>tips</strong>. Por fim, carregaremos a biblioteca matplotlib para visualizar o resultado do nosso processamento.\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "Mk-L-MqXyF63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "hYhxXuahyehy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Utilizaremos o conjunto de dados `tips` disponível na Seaborn nos exemplos. A célula abaixo carrega este conjunto de dados e realiza a remoção dos registros com valores vazios.\n",
        "</p>"
      ],
      "metadata": {
        "id": "ZWSFLBjPyjBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = sns.load_dataset('tips')\n",
        "df = df.dropna()\n",
        "\n",
        "df.info()"
      ],
      "metadata": {
        "id": "nOWOgctFynng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O exemplo abaixo apresenta a utilização da biblioteca NumPy para o cálculo de estatísticas básicas sobre um dos atributos do conjunto de dados. Observe que o atributo que está sendo analisado, <strong>total_bill</strong>, trata-se de um atributo do tipo float64, o que o torna compatível com os métodos do NumPy.\n",
        "</p>"
      ],
      "metadata": {
        "id": "UaLv0ytyy3BD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "media = np.mean(df['total_bill'])\n",
        "mediana = np.median(df['total_bill'])\n",
        "desvio_padrao = np.std(df['total_bill'])\n",
        "\n",
        "print(f\"Média: {media:.2f}\")\n",
        "print(f\"Mediana: {mediana:.2f}\")\n",
        "print(f\"Desvio Padrão: {desvio_padrao:.2f}\")"
      ],
      "metadata": {
        "id": "4Uy8o3y5y-Pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O SciPy oferece os métodos estatísticos necessários para o cálculo do z-score dos valores de um atributo. O z-score é também conhecido como valor padronizado e representa a distância em desvios padrão de um valor em relação à média de um conjunto de dados. Podemos aplicar o z-score como um método para a identificação de outliers em um atributo. Em suma, podemos calcular o z-score utilizando a seguinte fórmula:\n",
        "</p>\n",
        "$z = \\frac{x - \\mu}{\\sigma}$\n",
        "\n",
        "<p align=\"justify\">\n",
        "Onde $\\mu$ é a média do conjunto de dados, $\\sigma$ é o desvio padrão, $x$ é o valor do atributo para um registro, e $z$ é o valor resultante do z-score. Após calcular o z-score para cada valor, podemos utilizar um limiar para seu valor, por exemplo $|z| > 3$, para definir quais registros possuem outliers em seus valores.\n",
        "</p>\n",
        "<p align=\"justify\">\n",
        "A célula abaixo implementa a identificação de outliers utilizando o método do z-score implementado através do SciPy. Após identificar os outliers, estes registros serão marcados através de um novo atributo do tipo booleano.\n",
        "</p>"
      ],
      "metadata": {
        "id": "PJRS1EjZzIev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z_scores = sp.stats.zscore(df['total_bill']) # Cálculo do Z-Score\n",
        "\n",
        "# Identificando outliers\n",
        "threshold = 3\n",
        "df['outlier_z'] = np.abs(z_scores) > threshold\n",
        "\n",
        "print(\"Outliers identificados pelo Z-Score:\")\n",
        "df[df['outlier_z'] == True]"
      ],
      "metadata": {
        "id": "6XfoTVinzSr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "Também podemos utilizar o SciPy para realizar a análise de outliers através do método IQR, assim como implementado através da biblioteca Pandas. A célula abaixo apresenta o método IQR implementado através do SciPy.\n",
        "</p>"
      ],
      "metadata": {
        "id": "1rsoVDlY0Anu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cálculo dos quartis e IQR para 'total_bill'\n",
        "Q1 = df['total_bill'].quantile(0.25)\n",
        "Q3 = df['total_bill'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Limites para detecção de outliers\n",
        "limite_inferior = Q1 - 1.5 * IQR\n",
        "limite_superior = Q3 + 1.5 * IQR\n",
        "\n",
        "# Identificar outliers\n",
        "df['outlier_iqr'] = (df['total_bill'] < limite_inferior) | (df['total_bill'] > limite_superior)\n",
        "\n",
        "# Exibir registros marcados como outliers pelo IQR\n",
        "print(\"Outliers identificados pelo IQR:\")\n",
        "df[df['outlier_iqr'] == True]"
      ],
      "metadata": {
        "id": "_3pili_y0Gmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "A título de comparação dos dois métodos, podemos fazer uma análise visual dos outliers através de um gráfico de boxplot gerado através do Matplotlib e do Seaborn. A célula abaixo gera este gráfico para o atributo <strong>total_bill</strong>, que utilizamos nos métodos anteriores.\n",
        "</p>"
      ],
      "metadata": {
        "id": "6Jc1gqg-0hWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o boxplot tradicional\n",
        "plt.figure(figsize=(12, 4))\n",
        "sns.boxplot(x=df['total_bill'], color='lightblue')\n",
        "\n",
        "# Sobrepor os outliers Z-Score com pontos vermelhos. Observe que o padrão do boxplot é IQR (cinza)\n",
        "sns.stripplot(\n",
        "    x='total_bill',\n",
        "    data=df[df['outlier_z']],\n",
        "    color='red',\n",
        "    size=8,\n",
        "    marker='D',  # marcador em forma de losango\n",
        "    label='Outlier'\n",
        ")\n",
        "\n",
        "plt.title(\"Boxplot de 'total_bill' com Outliers\")\n",
        "plt.xlabel(\"Valor Total da Conta (total_bill)\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FH-RfezX0uPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6.Tratamento com Scikit-Learn\n",
        "<p align=\"justify\">\n",
        "O Scikit-Learn é uma biblioteca Python de código aberto para aprendizado de máquina que fornece ferramentas simples e eficientes para tarefas como classificação, regressão, clustering, redução de dimensionalidade e pré-processamento de dados. Iremos utilizar essa biblioteca ao longo de várias disciplinas onde iremos trabalhar com algoritmos de aprendizado. Além disso, o Scikit-Learn também oferece métodos para realizar a preparação e engenharia de atributos em conjunto de dados. O Scikit-Learn suporta nativamente as estruturas de dados disponibilizadas pelas bibliotecas Pandas e NumPy, facilitando a integração de seus métodos.\n",
        "</p>\n",
        "\n",
        "Para Modelos de machine learning:\n",
        "- Classificação (ex: SVM, k-NN, árvore de decisão, regressão logística)\n",
        "- Regressão (ex: regressão linear, ridge, Lasso)\n",
        "- Agrupamento (clustering) (ex: K-Means, DBSCAN)\n",
        "- Redução de dimensionalidade (ex: PCA)\n",
        "- Modelos baseados em ensemble (ex: Random Forest, Gradient Boosting)\n",
        "\n",
        "Ferramentas auxiliares:\n",
        "- Divisão de dados em treino/teste (train_test_split)\n",
        "- Validação cruzada (cross_val_score)\n",
        "- Pipeline de pré-processamento\n",
        "- Métricas de avaliação de modelos (acurácia, F1, etc.)\n",
        "- Normalização e padronização de dados\n",
        "\n",
        "<p align=\"justify\">\n",
        "Neste notebook iremos trabalhar com algumas das funções mais básicas que o Scikit-Learn oferece para a preparação de conjuntos de dados, em particular métodos para normalização de valores de atributos. Iremos utilizar o conjunto de dados <strong>tips</strong> disponível no Seaborn para realização das atividades. Primeiramente, iremos carregar as bibliotecas Pandas e Seaborn para importar o conjunto de dados em um Dataframe.\n",
        "</p>"
      ],
      "metadata": {
        "id": "iDlQnwZm4-nw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Xt4B7mkY5Q6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O Scikit-Learn implementa os métodos de normalização baseados em z-score e min-max. Diferentemente do Pandas, estes métodos já encontram-se implementados e prontos para utilização. Na célula abaixo iremos aplicar o método de normalização baseado em z-score, que é implementado na classe <strong>StandardScaler</strong> no Scikit-Learn.\n",
        "</p>"
      ],
      "metadata": {
        "id": "Sdw9zM2l5Ta2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar a classe StandardScaler para utilizar a normalização por z-score.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Carregar o dataset \"tips\" utilizando o Seaborn\n",
        "df = sns.load_dataset('tips')\n",
        "\n",
        "# Vamos criar um subconjunto do dataset \"tips\" contendo apenas as colunas que iremos normalizar\n",
        "X = df[['total_bill', 'tip', 'size']]\n",
        "\n",
        "# Agora criamos uma instância do StandardScaler e a utilizamos para normalizar os valores do subconjunto de dados\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Após o processo de normalização, podemos substituir os dados originais no conjunto pelos dados normalizados.\n",
        "df[['total_bill', 'tip', 'size']] = X_scaled\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wEvZ5Gfq5bKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<p align=\"justify\">\n",
        "O Scikit-Learn também possui uma implementação do método de normalização min-max. Seu uso requer a instanciação da classe <strong>MinMaxScaler</strong>, de modo similar ao exemplo anterior. A célula abaixo apresenta a utilização de normalização min-max com o Scikit-Learn.\n",
        "</p>"
      ],
      "metadata": {
        "id": "VS60P1ZC5yuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar a classe MinMaxScaler para utilizar a normalização por min-max.\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Carregar o dataset \"tips\" utilizando o Seaborn\n",
        "df = sns.load_dataset('tips')\n",
        "\n",
        "# Vamos criar um subconjunto do dataset \"tips\" contendo apenas as colunas que iremos normalizar\n",
        "X = df[['total_bill', 'tip', 'size']]\n",
        "\n",
        "# Agora criamos uma instância do MinMaxScaler e a utilizamos para normalizar os valores do subconjunto de dados\n",
        "normalizer = MinMaxScaler()\n",
        "X_normalized = normalizer.fit_transform(X)\n",
        "\n",
        "# Após o processo de normalização, podemos substituir os dados originais no conjunto pelos dados normalizados.\n",
        "df[['total_bill', 'tip', 'size']] = X_normalized\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3rh381Be58kW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}